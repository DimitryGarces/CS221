\item {\bf Q-Learning Mountain Car}

In the previous question, we've seen how value iteration can take an MDP which describes the full dynamics of the game 
and return an optimal policy, and we've also seen how model-based value iteration with Monte Carlo simulation can estimate MDP dynamics if unknown at first
and then learn the respective optimal policy. But suppose you are trying to control a complex system in the real world where trying to explicitly model
all possible transitions and rewards is intractable. We will see how model-free reinforcement learning can nevertheless find the optimal policy.

\begin{enumerate}

  \input{04-q-learning-mountain-car/01-implement}

  \input{04-q-learning-mountain-car/02-approx}

  \input{04-q-learning-mountain-car/03-q-value}

  \input{04-q-learning-mountain-car/04-simulation}

  \input{04-q-learning-mountain-car/05-benefits}

\end{enumerate}
